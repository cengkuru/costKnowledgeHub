# CoST DNA Principles Reference

The CoST DNA Analyzer evaluates every recommendation against four foundational pillars. Use this reference to understand the scoring rubric, risk triggers, and what “good” looks like inside each domain.

Each pillar is scored 0–10. Scores ≥8 indicate strong alignment, 5–7 signal mixed performance, and <5 require immediate remediation. Risk severity follows a low / medium / high scale.

---

## 1. Disclosure Transparency (`disclosureTransparency`)

- **Definition** – Public authorities publish timely, accessible information across the infrastructure project lifecycle.
- **Guiding questions**
  - Are all major milestones, contracts, and updates disclosed without delay?
  - Is information accessible in formats that communities and watchdogs can interrogate?
  - Does disclosure enable tracking of funds, progress, and contractor performance?
- **Positive signals**
  - Publication schedules spanning planning, procurement, delivery, and completion.
  - Open, structured data aligned to CoST IDS / OC4IDS.
  - Transparent change logs for variations and approvals.
- **Red flags**
  - Contract details missing or heavily redacted.
  - Release cycles that lag months behind real decisions.
  - PDF dumps or portals that exclude non-technical audiences.

---

## 2. Assurance Quality (`assuranceQuality`)

- **Definition** – Independent validation confirms disclosures are true, complete, and meaningful.
- **Guiding questions**
  - Are independent specialists reviewing published data?
  - Do assurance findings trigger corrective actions and public reporting?
  - Is there a transparent methodology for sampling, verification, and follow-up?
- **Positive signals**
  - Multidisciplinary, independent assurance teams.
  - Published findings tied to actionable improvement plans.
  - Public summaries that translate technical insights for wider audiences.
- **Red flags**
  - Sporadic or under-resourced assurance exercises.
  - Findings that never reach the public or fail to prompt changes.
  - Delivery teams assessing their own work.

---

## 3. Multi-Stakeholder Participation (`multiStakeholderParticipation`)

- **Definition** – Government, private sector, and civil society co-create decisions and oversight.
- **Guiding questions**
  - Do civil society and communities hold formal roles in governance structures?
  - Do private operators proactively support transparency commitments?
  - Is participation continuous from planning through delivery?
- **Positive signals**
  - Balanced governance bodies with civic and oversight representation.
  - Private sector partners co-own reporting obligations.
  - Feedback loops that demonstrate how citizen input shaped decisions.
- **Red flags**
  - Token consultations without feedback or follow-up.
  - Civic actors denied access to meaningful data.
  - Dominant stakeholders able to veto or ignore recommendations.

---

## 4. Social Accountability (`socialAccountability`)

- **Definition** – Communities can monitor delivery, raise grievances, and secure remedies.
- **Guiding questions**
  - Do citizens have real-time channels to monitor performance and costs?
  - Are grievance mechanisms accessible, responsive, and protective of whistleblowers?
  - Do accountability outcomes loop back into planning and procurement reforms?
- **Positive signals**
  - Citizen monitors or scorecards integrated into delivery oversight.
  - Public dashboards sharing costs, milestones, and commitments.
  - Documented grievance resolutions and systemic fixes.
- **Red flags**
  - Communities fear retaliation for speaking up.
  - Complaints vanish without transparent handling.
  - Insights from accountability never reach decision-makers.

---

## Stakeholder Balance & Power Dynamics

- **Stakeholder balance map** – Analyzer classifies representation for `government`, `privateSector`, `civilSociety`, `beneficiaries`, and `oversightBodies` as `strong`, `balanced`, or `underrepresented`.
- **Power dynamics insights** – Highlights whose interests are being served, who is left out, and proposes mitigation steps (e.g., “Bring civil society into assurance planning workshops”).

Use these cues to interpret analyzer output, set thresholds for automated gating, or brief teams on where to focus remediation.
